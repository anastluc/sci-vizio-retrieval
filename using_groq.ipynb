{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "import io\n",
    "from PIL import Image\n",
    "import os\n",
    "import logging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_URL = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "GROQ_API_KEY= \"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\"\n",
    "\n",
    "with open('./diagram1.png', 'rb') as img:\n",
    "    image_bytes = img.read()\n",
    "    encoded = base64.b64encode(image_bytes).decode('utf-8')\n",
    "\n",
    "    # Validate image format\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(image_bytes))\n",
    "        img.verify()\n",
    "    except Exception as e:\n",
    "        print(f\"Invalid image format: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the messages for Groq API\n",
    "query = \"Describe the image in json format. This json should have a title, description and detected data attirbutes\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": query},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded}\"}}\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make API request\n",
    "def make_api_request(model):\n",
    "    response = requests.post(\n",
    "        GROQ_API_URL,\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": 1000\n",
    "        },\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        timeout=30\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_response = make_api_request(\"llama-3.2-11b-vision-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-f7005018-309f-4032-93e8-7971224f5f7e',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1731335528,\n",
       " 'model': 'llama-3.2-11b-vision-preview',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '**1. What is the image about?**\\n\\nThe image shows a simple presentation of \"Which mammal catches mice, enjoys eating fish and has a tail?\"\\n\\n**2. How would you create a description of \"at\" with 10-30 words?**\\n\\nAnswer: This image describes \"at\" with the description that \"at\" is a mammal with a tail, primarily feeds on mice. It is a cat.\\n\\n**3. How would you detect the category of the picture provided?**\\n\\nThe image can be detected as a **NNP (Noun, Proper)** as \"It is a cat.\" \\nThese categories are based on the provided words in the prompt.\\n\\n**4. How can you describe the image through JSON in order to have all your data described at one set?**\\n\\nBelow is a proposed JSON key-value pair. Answer: \\n```\\n{\\n    \"title\": \"at\",\\n    \"description\": \"It is a cat\",\\n    \"NNP\": {\"NLP\": \"It is a cat.\",\"CV\": \"a mammal with a tail, primarily feeds on mice.\"},\\n    \"NN\": {\"RAG\": \"Please help me. Similar photos:\"},\\n    \"NNB\": {\"RAG\": \"Cats that eat fish, have a tail, feed on mice..\"},\\n    \"Cat\": {\"RAG\": \"a mammal with a tail, primarily feeds on mice.\"}\\n}\\n```\\n\\n**5. Why are the categories [NNP, NN, NNB, Cat] for the detected data in the image?**\\n\\n**a. NNP:** In the prompt, the words \"at,\"  and \"it\" are singular, and \"is\" is a linking verb. \\n\\nNN should stand for a proper noun and would include the images of the different cats. NN would stand for proper nouns with no spaces and B would indicate with no spaces. These need to be separated properly and would not fully describe all the cases in the image.\\n\\n**b. NN:** The proper nouns need to match exactly as written, which seems to match all the words in this situation.\\n\\nc. **NNB:** When making comparisons like or, the same or different, NNB would stand for non-Boolean based non-proper nouns. This includes words such as, is  this image, type cat, NNB not known as the proper or image as not in the sentence but the cat and what you are wearing can be detected. This will help differentiate between pronouns, is, and for words such as type indicating specific type of noun.\\n\\n**d. Cat:** One can find examples and matching the information from the prompt based on specific singular cats showing in the image.\\n\\n**6. What is the data in the order of categories?**\\n\\nThe data in the order of categories as seen in the image is: animal cat mammal with tail, primarily feeds on mice, and eats fish.'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'queue_time': 0.219665554,\n",
       "  'prompt_tokens': 33,\n",
       "  'prompt_time': 0.002370624,\n",
       "  'completion_tokens': 592,\n",
       "  'completion_time': 0.950687978,\n",
       "  'total_tokens': 625,\n",
       "  'total_time': 0.953058602},\n",
       " 'system_fingerprint': 'fp_fa3d3d25b0',\n",
       " 'x_groq': {'id': 'req_01jcdt77apfr2re38gqe0ekc22'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**1. What is the image about?**\\n\\nThe image shows a simple presentation of \"Which mammal catches mice, enjoys eating fish and has a tail?\"\\n\\n**2. How would you create a description of \"at\" with 10-30 words?**\\n\\nAnswer: This image describes \"at\" with the description that \"at\" is a mammal with a tail, primarily feeds on mice. It is a cat.\\n\\n**3. How would you detect the category of the picture provided?**\\n\\nThe image can be detected as a **NNP (Noun, Proper)** as \"It is a cat.\" \\nThese categories are based on the provided words in the prompt.\\n\\n**4. How can you describe the image through JSON in order to have all your data described at one set?**\\n\\nBelow is a proposed JSON key-value pair. Answer: \\n```\\n{\\n    \"title\": \"at\",\\n    \"description\": \"It is a cat\",\\n    \"NNP\": {\"NLP\": \"It is a cat.\",\"CV\": \"a mammal with a tail, primarily feeds on mice.\"},\\n    \"NN\": {\"RAG\": \"Please help me. Similar photos:\"},\\n    \"NNB\": {\"RAG\": \"Cats that eat fish, have a tail, feed on mice..\"},\\n    \"Cat\": {\"RAG\": \"a mammal with a tail, primarily feeds on mice.\"}\\n}\\n```\\n\\n**5. Why are the categories [NNP, NN, NNB, Cat] for the detected data in the image?**\\n\\n**a. NNP:** In the prompt, the words \"at,\"  and \"it\" are singular, and \"is\" is a linking verb. \\n\\nNN should stand for a proper noun and would include the images of the different cats. NN would stand for proper nouns with no spaces and B would indicate with no spaces. These need to be separated properly and would not fully describe all the cases in the image.\\n\\n**b. NN:** The proper nouns need to match exactly as written, which seems to match all the words in this situation.\\n\\nc. **NNB:** When making comparisons like or, the same or different, NNB would stand for non-Boolean based non-proper nouns. This includes words such as, is  this image, type cat, NNB not known as the proper or image as not in the sentence but the cat and what you are wearing can be detected. This will help differentiate between pronouns, is, and for words such as type indicating specific type of noun.\\n\\n**d. Cat:** One can find examples and matching the information from the prompt based on specific singular cats showing in the image.\\n\\n**6. What is the data in the order of categories?**\\n\\nThe data in the order of categories as seen in the image is: animal cat mammal with tail, primarily feeds on mice, and eats fish.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_llama_response = make_api_request(\"llama-3.2-90b-vision-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The image is a flowchart that illustrates the process of determining what type of mammal a user is looking for based on their query. The flowchart begins with a question, \"Which mammal catches mice, enjoys eating fish and has a tail?\" and branches out into different possible answers.\\n\\nTo represent this image as JSON data with title, description, and detected data tags, we can use the following structure:\\n\\n```\\n{\\n  \"title\": \"Mammal Identification Flowchart\",\\n  \"description\": \"This flowchart helps identify the type of mammal based on user queries.\",\\n  \"detectedData\": [\\n    {\\n      \"query\": \"Which mammal catches mice, enjoys eating fish and has a tail?\",\\n      \"answer\": \"Cat\"\\n    },\\n    {\\n      \"query\": \"Please help me inpaint the picture.\",\\n      \"answer\": \"Similar photos\"\\n    },\\n    {\\n      \"query\": \"What will this user buy next?\",\\n      \"answer\": \"Maybe a T-shirt.\"\\n    }\\n  ]\\n}\\n```\\n\\nThis JSON object has three main attributes: `title`, `description`, and `detectedData`. The `title` attribute represents the title of the flowchart, while the `description` attribute provides a brief description of the flowchart\\'s purpose. The `detectedData` attribute is an array of objects, each representing a query and its corresponding answer.\\n\\nEach object in the `detectedData` array has two attributes: `query` and `answer`. The `query` attribute represents the user\\'s query, while the `answer` attribute represents the answer provided by the flowchart.\\n\\nOverall, this JSON representation accurately reflects the content and structure of the image, including the title, description, and detected data tags.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_llama_response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_api_request(model, messages):\n",
    "    response = requests.post(\n",
    "        GROQ_API_URL,\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": 1000\n",
    "        },\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        timeout=30\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Return a json containing as much information as possible from this diagram. It should contain a description of what type of image is (e.g. diagram, graph, flowchart, etc.) and the data that comprises it\"\"\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": query},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded}\"}}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "res = make_api_request(model=\"llama-3.2-90b-vision-preview\", messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Diagram Analysis**\\n\\n**Image Description:**\\nThe image is a flowchart.\\n\\n**Flowchart Components:**\\n\\n*   **Questions:** \\n    *   \"Which mammal catches mice, enjoys eating fish and has a tail?\"\\n    *   \"Please help me inpaint the picture.\"\\n    *   \"What will this user buy next?\"\\n*   **RAG (Retrieval-Augmented Generation):**\\n    *   The flowchart includes RAG boxes that generate answers to the questions based on the provided information.\\n*   **Data Points:**\\n    *   Mammals (cats, mice)\\n    *   Actions (catching, eating, inpainting)\\n    *   User preferences (buying)\\n\\n**Flowchart Structure:**\\n\\n*   **Arrows:** The flowchart uses arrows to connect questions to RAG boxes and then to answer boxes.\\n*   **Decision Points:** There are decision points between RAG boxes and answer boxes, indicating conditional logic.\\n\\n**Answer Boxes:**\\n\\n*   **Text Answers:** The answer boxes contain text responses to the questions.\\n*   **Images:** Some answer boxes include images (e.g., a cat, a t-shirt) to supplement the text answers.\\n\\n**Conclusion:**\\nThe flowchart is a decision-making tool that uses RAG boxes to generate answers to questions based on provided data points. The chart\\'s structure and components work together to provide a clear and organized visual representation of the decision-making process.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output_format = \"\"\"\n",
    "{\n",
    "  \"image_type\": \"Graph\",\n",
    "  \"title\": \"French composite indicator and year-on-year GDP growth rate\",\n",
    "  \"time_period\": \"1992 Q1 - 2010 Q1\",\n",
    "  \"x_axis\": {\n",
    "    \"label\": \"Year\",\n",
    "    \"data_range\": \"1992 Q1 to 2010 Q1\",\n",
    "    \"ticks\": [\n",
    "      \"1992 Q1\",\n",
    "      \"1994 Q1\",\n",
    "      \"1996 Q1\",\n",
    "      \"1998 Q1\",\n",
    "      \"2000 Q1\",\n",
    "      \"2002 Q1\",\n",
    "      \"2004 Q1\",\n",
    "      \"2006 Q1\",\n",
    "      \"2008 Q1\",\n",
    "      \"2010 Q1\"\n",
    "    ]\n",
    "  },\n",
    "  \"y_axis_left\": {\n",
    "    \"label\": \"Balance of opinion (%)\",\n",
    "    \"range\": \"0 to 140\",\n",
    "    \"line\": {\n",
    "      \"type\": \"Solid\",\n",
    "      \"label\": \"Composite indicator\"\n",
    "    }\n",
    "  },\n",
    "  \"y_axis_right\": {\n",
    "    \"label\": \"Year-on-year (y-o-y) GDP growth (%)\",\n",
    "    \"range\": \"-5.00 to 3.58\",\n",
    "    \"line\": {\n",
    "      \"type\": \"Dashed\",\n",
    "      \"label\": \"y-o-y GDP growth rate\"\n",
    "    }\n",
    "  },\n",
    "  \"key_patterns\": [\n",
    "    {\n",
    "      \"period\": \"1992-1993\",\n",
    "      \"observation\": \"Both indicators start declining sharply.\"\n",
    "    },\n",
    "    {\n",
    "      \"period\": \"1994-2001\",\n",
    "      \"observation\": \"Composite indicator fluctuates between 80 and 120, GDP growth between -1% and 3%.\"\n",
    "    },\n",
    "    {\n",
    "      \"period\": \"2008-2009\",\n",
    "      \"observation\": \"Sharp decline in both the composite indicator and GDP growth during the global financial crisis.\"\n",
    "    },\n",
    "    {\n",
    "      \"period\": \"2010\",\n",
    "      \"observation\": \"Composite indicator and GDP growth rebound sharply after 2009.\"\n",
    "    }\n",
    "  ],\n",
    "  \"sources\": [\n",
    "    \"<a source of the data used in this diagram>\"\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "USER_PROMPT = \"\"\"You will be provided with an image. \n",
    "Your response should contain as much information as possible from this diagram. \n",
    "It should contain a description of what type of image is (e.g. diagram, graph, flowchart, etc.) \n",
    "and the data that comprises it.\n",
    "The response should be a JSON object of the following format:\n",
    "{json_output_format} \n",
    "Begin immediately with outputting the JSON object, do NOT prefix with any extra text, start straight with the json object, i.e. the first character should be {{\n",
    "Do NOT suffix with any extra text, finish with the json object, i.e. the last character should be }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": USER_PROMPT},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded}\"}}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "res = make_api_request(model=\"llama-3.2-90b-vision-preview\", messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"type\": \"flowchart\",\\n    \"data\": {\\n        \"title\": \"Query\",\\n        \"sections\": [\\n            {\\n                \"section_title\": \"Which mammal catches mice, enjoys eating fish and has a tail?\",\\n                \"query\": \"What is the mammal that catches mice, enjoys eating fish and has a tail?\",\\n                \"answer\": \"It is a cat.\",\\n                \"rationale\": \"A cat is a mammal that catches mice, enjoys eating fish and has a tail.\"\\n            },\\n            {\\n                \"section_title\": \"Please help me inpaint the picture.\",\\n                \"query\": \"What will this user buy next?\",\\n                \"answer\": \"Maybe a T-shirt.\",\\n                \"rationale\": \"The user has purchased a cat-related item and may be interested in purchasing a T-shirt as well.\"\\n            },\\n            {\\n                \"section_title\": \"What will this user buy next?\",\\n                \"query\": \"What is the predicted purchase of this user based on their previous purchases?\",\\n                \"answer\": \"Maybe a T-shirt.\",\\n                \"rationale\": \"The user has purchased a cat-related item and may be interested in purchasing a T-shirt as well.\"\\n            }\\n        ]\\n    }\\n}'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'flowchart',\n",
       " 'data': {'title': 'Query',\n",
       "  'sections': [{'section_title': 'Which mammal catches mice, enjoys eating fish and has a tail?',\n",
       "    'query': 'What is the mammal that catches mice, enjoys eating fish and has a tail?',\n",
       "    'answer': 'It is a cat.',\n",
       "    'rationale': 'A cat is a mammal that catches mice, enjoys eating fish and has a tail.'},\n",
       "   {'section_title': 'Please help me inpaint the picture.',\n",
       "    'query': 'What will this user buy next?',\n",
       "    'answer': 'Maybe a T-shirt.',\n",
       "    'rationale': 'The user has purchased a cat-related item and may be interested in purchasing a T-shirt as well.'},\n",
       "   {'section_title': 'What will this user buy next?',\n",
       "    'query': 'What is the predicted purchase of this user based on their previous purchases?',\n",
       "    'answer': 'Maybe a T-shirt.',\n",
       "    'rationale': 'The user has purchased a cat-related item and may be interested in purchasing a T-shirt as well.'}]}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json_object = json.loads(res.json()[\"choices\"][0][\"message\"][\"content\"])\n",
    "json_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m res \u001b[38;5;241m=\u001b[39m make_api_request(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.2-90b-vision-preview\u001b[39m\u001b[38;5;124m\"\u001b[39m, messages\u001b[38;5;241m=\u001b[39mmessages)\n\u001b[1;32m     24\u001b[0m json_string \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 25\u001b[0m json_object \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m json_object\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/lib/python3.11/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "USER_PROMPT = \"\"\"You will be provided with an image. \n",
    "Your response should contain as much information as possible from this diagram. \n",
    "It should contain a description of what type of image is (e.g. diagram, graph, flowchart, etc.) \n",
    "and the data that comprises it.\n",
    "The response should be a valid JSON object as a string. \n",
    "The JSON should necessarily have the following attributes:\n",
    "image_type, title, description\n",
    "But also if applicable the following:\n",
    "time_period, x-axis, y-axis, sources, sections, labels, ticks, key patterns\n",
    "An example JSON of the desired format is the following:\n",
    "{json_output_format}\n",
    "Begin immediately with outputting the JSON object, do NOT prefix with any extra text, start straight with the json object, i.e. the first character should be {{\n",
    "Do NOT suffix with any extra text, finish with the json object, i.e. the last character should be }\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": USER_PROMPT},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded}\"}}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "res = make_api_request(model=\"llama-3.2-90b-vision-preview\", messages=messages)\n",
    "json_string = res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "json_object = json.loads(json_string)\n",
    "json_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm not able to provide any details of the image as I am unable to see it. Please provide the image or describe it to me so I can assist you better.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Step 1: Identify the Image Type**\\nThe image appears to be a flowchart, which is a type of diagram that represents a process or methodology.\\n\\n**Step 2: Extract Title and Description**\\nThe title of the image is not explicitly provided. However, based on the content, a possible description could be \"Flowchart illustrating the process of determining user preference for purchasing a T-shirt or similar items.\"\\n\\n**Step 3: Identify Relevant Sections or Labels**\\nThe flowchart consists of several sections or boxes, each containing text or images. The main sections are:\\n\\n*   **Query**: This section asks questions to determine user preferences.\\n*   **Answer**: This section provides answers to the questions asked in the Query section.\\n*   **RAG**: This section contains a red, amber, and green (RAG) rating system, which is used to evaluate user preferences.\\n\\n**Step 4: Extract Data**\\nThe flowchart contains various data points, including:\\n\\n*   **User Preferences**: The flowchart aims to determine user preferences for purchasing a T-shirt or similar items.\\n*   **Product Types**: The flowchart mentions different types of products, such as T-shirts, cats, and mice.\\n*   **Ratings**: The RAG rating system is used to evaluate user preferences, with red indicating low preference, amber indicating moderate preference, and green indicating high preference.\\n\\n**Step 5: Assemble JSON Object**\\nBased on the extracted information, the JSON object can be assembled as follows:\\n\\n```\\n{\\n    \"image_type\": \"flowchart\",\\n    \"title\": \"\",\\n    \"description\": \"Flowchart illustrating the process of determining user preference for purchasing a T-shirt or similar items.\",\\n    \"sections\": [\\n        {\\n            \"name\": \"Query\",\\n            \"description\": \"This section asks questions to determine user preferences.\"\\n        },\\n        {\\n            \"name\": \"Answer\",\\n            \"description\": \"This section provides answers to the questions asked in the Query section.\"\\n        },\\n        {\\n            \"name\": \"RAG\",\\n            \"description\": \"This section contains a red, amber, and green (RAG) rating system, which is used to evaluate user preferences.\"\\n        }\\n    ],\\n    \"data\": [\\n        {\\n            \"type\": \"user_preferences\",\\n            \"description\": \"The flowchart aims to determine user preferences for purchasing a T-shirt or similar items.\"\\n        },\\n        {\\n            \"type\": \"product_types\",\\n            \"description\": \"The flowchart mentions different types of products, such as T-shirts, cats, and mice.\"\\n        },\\n        {\\n            \"type\": \"ratings\",\\n            \"description\": \"The RAG rating system is used to evaluate user preferences, with red indicating low preference, amber indicating moderate preference, and green indicating high preference.\"\\n        }\\n    ]\\n}\\n```\\n\\n**Answer**: \\n\\n```\\n{\\n    \"image_type\": \"flowchart\",\\n    \"title\": \"\",\\n    \"description\": \"Flowchart illustrating the process of determining user preference for purchasing a T-shirt or similar items.\",\\n    \"sections\": [\\n        {\\n            \"name\": \"Query\",\\n            \"description\": \"This section asks questions to determine user preferences.\"\\n        },\\n        {\\n            \"name\": \"Answer\",\\n            \"description\": \"This section provides answers to the questions asked in the Query section.\"\\n        },\\n        {\\n            \"name\": \"RAG\",\\n            \"description\": \"This section contains a red, amber, and green (RAG) rating system, which is used to evaluate user preferences.\"\\n        }\\n    ],\\n    \"data\": [\\n        {\\n            \"type\": \"user_preferences\",\\n            \"description\": \"The flowchart aims to determine user preferences for purchasing a T-shirt or similar items.\"\\n        },\\n        {\\n            \"type\": \"product_types\",\\n            \"description\": \"The flowchart mentions different types of products, such as T-shirts, cats, and mice.\"\\n        },\\n        {\\n            \"type\": \"ratings\",\\n            \"description\": \"The RAG rating system is used to evaluate user preferences, with red indicating low preference, amber indicating moderate preference, and green indicating high preference.\"\\n        }\\n    ]\\n}\\n```'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
