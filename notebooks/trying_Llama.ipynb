{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import base64\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_base64(image_path):\n",
    "    # Open the image file\n",
    "    with Image.open(image_path) as img:\n",
    "        # Create a BytesIO object to hold the image data\n",
    "        buffered = io.BytesIO()\n",
    "        # Save the image to the BytesIO object in a specific format (e.g., PNG)\n",
    "        img.save(buffered, format=\"PNG\")\n",
    "        # Get the byte data from the BytesIO object\n",
    "        img_bytes = buffered.getvalue()\n",
    "        # Encode the byte data to base64\n",
    "        img_base64 = base64.b64encode(img_bytes).decode('utf-8')\n",
    "        return img_base64\n",
    "\n",
    "# Example usage\n",
    "image_path = 'diagram1.png'  # Replace with your image path\n",
    "base64_image = image_to_base64(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response: The image presents a flowchart illustrating the process of generating text based on user input. The flowchart is divided into three sections, each representing a different stage in the text generation process.\n",
      "\n",
      "*   **User Input**\n",
      "    *   The first section of the flowchart begins with a box labeled \"Query\" that contains an example question: \"Which mammal catches mice, enjoys eating fish and has a tail?\"\n",
      "    *   Below this box is another labeled \"RAG\" (Reinforcement Learning from Augmented Data), which indicates that the user's input will be processed using a reinforcement learning algorithm.\n",
      "*   **Text Generation**\n",
      "    *   The second section of the flowchart shows how the RAG model processes the user's input and generates text based on it.\n",
      "    *   The box labeled \"RAG\" is connected to two other boxes: one labeled \"Answer\" and another labeled \"Similar Photos\".\n",
      "    *   The \"Answer\" box contains a response generated by the RAG model, which in this case is \"Cat, a mammal with a tail, primarily feeds on mice.\"\n",
      "    *   The \"Similar Photos\" box shows an image of a cat.\n",
      "*   **User Feedback**\n",
      "    *   The third section of the flowchart illustrates how the user can provide feedback to improve the accuracy of the generated text.\n",
      "    *   A box labeled \"RAG\" is connected to another box labeled \"Answer\", which contains a response generated by the RAG model, such as \"Maybe a T-shirt.\"\n",
      "    *   Below this box is an arrow pointing downwards, indicating that the user can provide feedback on the accuracy of the generated text.\n",
      "\n",
      "In summary, the flowchart illustrates how the RAG model processes user input and generates text based on it. The user can then provide feedback to improve the accuracy of the generated text.\n"
     ]
    }
   ],
   "source": [
    "# Use Ollama to analyze the image with Llama 3.2-Vision\n",
    "import ollama\n",
    "response = ollama.chat(\n",
    "    model=\"x/llama3.2-vision:11b\",\n",
    "    messages=[{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Describe this image?\",\n",
    "      \"images\": [base64_image]\n",
    "    }],\n",
    ")\n",
    "\n",
    "# Extract the model's response about the image\n",
    "cleaned_text = response['message']['content'].strip()\n",
    "print(f\"Model Response: {cleaned_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response: The image presents a graph with two line charts, each displaying the accuracy of protein and enzyme datasets in relation to their respective k values. The purpose of the graph is to compare the performance of these two types of datasets.\n",
      "\n",
      "* Two line charts are shown:\n",
      "\t+ One chart shows the accuracy of proteins\n",
      "\t+ The other chart shows the accuracy of enzymes\n",
      "* The x-axis represents the k value, ranging from 0 to 5.\n",
      "* The y-axis represents the accuracy percentage, ranging from 40% to 80%.\n",
      "* Each chart has a distinct color scheme:\n",
      "\t+ Proteins are represented by blue lines\n",
      "\t+ Enzymes are represented by red lines\n",
      "\n",
      "The main finding from this graph is that both proteins and enzymes have varying levels of accuracy across different k values. The accuracy of proteins generally increases as the k value increases, while the accuracy of enzymes decreases as the k value increases. This suggests that there may be differences in how these two types of datasets perform under different conditions.\n"
     ]
    }
   ],
   "source": [
    "image_path = 'diagram2.png'  # Replace with your image path\n",
    "base64_image = image_to_base64(image_path)\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"x/llama3.2-vision:11b\",\n",
    "    messages=[{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Describe this image?\",\n",
    "      \"images\": [base64_image]\n",
    "    }],\n",
    ")\n",
    "\n",
    "# Extract the model's response about the image\n",
    "cleaned_text = response['message']['content'].strip()\n",
    "print(f\"Model Response: {cleaned_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response: The image presents two line graphs that illustrate the accuracy of hyper-parameter studies on node classification with proteins and enzymes datasets. The graphs are titled \"Figure 3: Hyper-parameter study with hopsk (Left) from 1 to 5 and topK (Right) on node classification with PROTEINS, and ENZYMES datasets with the setting in Table 1.\"\n",
      "\n",
      "**Graph 1: Hopsk (Left)**\n",
      "\n",
      "*   The x-axis represents the number of hops (from 1 to 5).\n",
      "*   The y-axis represents accuracy (%).\n",
      "*   Two lines are plotted:\n",
      "    *   One line represents proteins, which starts at around 55% and increases to approximately 75%.\n",
      "    *   Another line represents enzymes, which begins at about 65% and decreases to roughly 60%.\n",
      "\n",
      "**Graph 2: TopK (Right)**\n",
      "\n",
      "*   The x-axis represents the top K value (from 1 to 5).\n",
      "*   The y-axis represents accuracy (%).\n",
      "*   Two lines are plotted:\n",
      "    *   One line represents proteins, which starts at around 50% and increases to approximately 70%.\n",
      "    *   Another line represents enzymes, which begins at about 60% and decreases to roughly 55%.\n",
      "\n",
      "**Key Observations**\n",
      "\n",
      "*   Both graphs show an increase in accuracy for proteins as the number of hops or top K value increases.\n",
      "*   Enzymes exhibit a decrease in accuracy as the number of hops or top K value increases.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "The image suggests that hyper-parameter tuning can significantly impact the performance of node classification models on protein and enzyme datasets. By adjusting the number of hops or top K value, researchers can optimize their models to achieve better results. However, it is essential to note that these findings are based on a specific study and may not generalize to other datasets or scenarios. Further research is necessary to fully understand the implications of hyper-parameter tuning in node classification tasks.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "    model=\"x/llama3.2-vision:11b\",\n",
    "    messages=[{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Describe the image in json format. This json should have a title, description and detected data attirbutes\",\n",
    "      \"images\": [base64_image]4\n",
    "    }],\n",
    ")\n",
    "\n",
    "# Extract the model's response about the image\n",
    "cleaned_text = response['message']['content'].strip()\n",
    "print(f\"Model Response: {cleaned_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
